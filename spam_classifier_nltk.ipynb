{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Mail Classifier using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.util import apply_features\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    0\n",
       "spam    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.761173\n",
       "1    0.238827\n",
       "Name: spam, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['spam'].value_counts()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train and test sets\n",
    "\n",
    "Because of presence of class imbalance, we also need to stratify while splitting our data. Our original dataset contains 76% spam and 24% non-spam values. Our train and test sets should contain similar class weightage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(data[\"text\"].values,\n",
    "                                                    data[\"spam\"].values,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 50,\n",
    "                                                    shuffle = True,\n",
    "                                                    stratify=data[\"spam\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the train_Y array:\n",
      "[[   0    1]\n",
      " [3270 1026]]\n",
      "[0.76117318 0.23882682]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(train_Y, return_counts=True)\n",
    "perc_elements = counts_elements/len(train_Y)\n",
    "print(\"Frequency of unique values of the train_Y array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "print(perc_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the test_Y array:\n",
      "[[   0    1]\n",
      " [1090  342]]\n",
      "[0.76117318 0.23882682]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(test_Y, return_counts=True)\n",
    "perc_elements = counts_elements/len(test_Y)\n",
    "print(\"Frequency of unique values of the test_Y array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "print(perc_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Classifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifier:\n",
    "\n",
    "    word_features = []      # collection of unique words\n",
    "    classifier = None       # placeholder variable for trained model\n",
    "    \n",
    "    def extract_tokens(self, text, target):\n",
    "        \"\"\"returns array of tuples where each tuple is defined by (tokenized_text, label)\n",
    "         parameters:\n",
    "                text: array of texts\n",
    "                target: array of target labels\n",
    "                \n",
    "        NOTE: consider only those words which have all alphabets and atleast 3 characters.\n",
    "        \"\"\"\n",
    "        tup_array = []\n",
    "        lem = WordNetLemmatizer()\n",
    "        stop_words_eng = set(stopwords.words('english'))\n",
    "        for i in range(len(text)):\n",
    "            tokenized_text = [lem.lemmatize(x.lower()) for x in word_tokenize(text[i]) \n",
    "                              if len(x)>=3 \n",
    "                                  and x.isalpha()\n",
    "                                  and x not in stop_words_eng\n",
    "                             ]\n",
    "            label = target[i]\n",
    "            tup_array.append(tuple([tokenized_text, label]))\n",
    "        return tup_array\n",
    "        \n",
    "    \n",
    "    def get_features(self, corpus):\n",
    "        \"\"\" \n",
    "        returns a Set of unique words in complete corpus. \n",
    "        parameters:- corpus: tokenized corpus along with target labels\n",
    "        \n",
    "        Return Type is a set\n",
    "        \"\"\"\n",
    "        all_words = []\n",
    "        for tokens, _ in corpus:\n",
    "            for word in tokens:\n",
    "                all_words.append(word)\n",
    "        \n",
    "        fdist = FreqDist(all_words)\n",
    "        uniqueWords = [x[0] for x in fdist.most_common(2000)]\n",
    "        return uniqueWords\n",
    "    \n",
    "    def extract_features(self, document):\n",
    "        \"\"\"\n",
    "        maps each input text into feature vector\n",
    "        parameters:- document: string\n",
    "        \n",
    "        Return type : A dictionary with keys being the train data set word features.\n",
    "                      The values correspond to True or False\n",
    "        \"\"\"\n",
    "        feature_vector = dict()\n",
    "        for word in self.word_features:\n",
    "            if word in document:\n",
    "                feature_vector[word] = True\n",
    "            else:\n",
    "                feature_vector[word] = False\n",
    "        return feature_vector\n",
    "\n",
    "    def train(self, text, labels):\n",
    "        \"\"\"\n",
    "        Returns trained model and set of unique words in training data\n",
    "        also set trained model to 'self.classifier' variable and set of \n",
    "        unique words to 'self.word_features' variable.\n",
    "        \"\"\"\n",
    "        trainTokens = self.extract_tokens(text, labels)\n",
    "        self.word_features = self.get_features(trainTokens)\n",
    "        train_feature_vectors = [(self.extract_features(d), c) for (d,c) in trainTokens]\n",
    "        self.classifier = NaiveBayesClassifier.train(train_feature_vectors)\n",
    "#         return self.classifier, self.word_features\n",
    "        \n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        Returns prediction labels of given input text. \n",
    "        Allowed Text can be a simple string i.e one input email, a list of emails, or a dictionary of emails identified by their labels.\n",
    "        \"\"\"\n",
    "        # tokenize\n",
    "        tokenized_test_X = []\n",
    "        lem = WordNetLemmatizer()\n",
    "        for i in range(len(text)):\n",
    "            tokenized_text = [lem.lemmatize(x) for x in word_tokenize(text[i]) if len(x)>=3 and x.isalpha()]\n",
    "            tokenized_test_X.append(tokenized_text)\n",
    "        \n",
    "        # create feature vectors\n",
    "        test_feature_vectors = [self.extract_features(x) for x in tokenized_test_X]\n",
    "        test_results = self.classifier.classify_many(test_feature_vectors)\n",
    "        \n",
    "        return test_results, test_feature_vectors\n",
    "    \n",
    "    def accuracy(self, feature_vectors, actual_labels):\n",
    "        \"\"\"\n",
    "        Returns accuracy.\n",
    "        Input Parameters: Feature vectors for test data strings, Actual labels for test set\n",
    "        To be called after running Predict function\n",
    "        \"\"\"\n",
    "        vector_labels = list(zip(feature_vectors, actual_labels))\n",
    "        acc = accuracy(self.classifier, vector_labels)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Object of Model Class and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SpamClassifier()\n",
    "classifier.train(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions and Test Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, feature_vectors = classifier.predict(test_X)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy is 94.69\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Accuracy is {round(classifier.accuracy(feature_vectors, test_Y)*100,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model gives ~95% accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
